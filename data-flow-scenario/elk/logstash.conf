input {
  kafka {
    bootstrap_servers => "kafka_broker:9093"
    topics => ["my-script-logs"]
    group_id => "logstash-consumer-group"
    codec => "plain"
    auto_offset_reset => "latest"
  }
}

filter {
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:syslog_timestamp} %{HOSTNAME:hostname} %{USER:user} %{WORD:process} : %{TIMESTAMP_ISO8601:app_time} hello devops : %{GREEDYDATA:json_string}"
    }
    tag_on_failure => ["grok_failure"]
  }

  if "grok_failure" not in [tags] {
    json {
      source => "json_string"
      tag_on_failure => ["json_failure"]
    }

    if "json_failure" in [tags] {
      mutate {
        add_tag => ["json_parse_error"]
      }
    }

    mutate {
      convert => {
        "[iran][population]" => "integer"
        "[iran][men]" => "integer"
        "[iran][women]" => "integer"
        "[iran][hOffset]" => "integer"
        "[iran][vOffset]" => "integer"
      }
    }

    date {
      match => ["syslog_timestamp", "ISO8601"]
      target => "@timestamp"
      timezone => "Asia/Tehran"
    }

    mutate {
      add_field => { "app_timestamp" => "%{app_time}" }
    }

    date {
      match => ["app_timestamp", "yyyy-MM-dd HH:mm:ss"]
      target => "app_timestamp"
      timezone => "Asia/Tehran"
      remove_field => ["app_time"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
    ilm_enabled => true
  }

  stdout {
    codec => rubydebug
  }
}
